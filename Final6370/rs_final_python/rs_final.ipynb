{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hls4ml version: 0.9.0.dev5+g033d4382\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "print(\"hls4ml version:\", hls4ml.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import qkeras\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "#from callbacks import all_callbacks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to [0, 1]\n",
    "x_train = np.round(x_train.astype(\"float32\") / 255).astype(\"int\")\n",
    "x_test = np.round(x_test.astype(\"float32\") / 255).astype(\"int\")\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "trim = 4\n",
    "x_train_ = x_train[:, trim:28-trim, trim:28-trim].reshape(x_train.shape[0],-1)\n",
    "x_test_ = x_test[:, trim:28-trim, trim:28-trim].reshape(x_test.shape[0],-1)\n",
    "input_shape = x_train_.shape[1:]\n",
    "print(input_shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b0000000000000000000000000000000000000000000000111110000000000000111001111000000000011000000111000000001000011100110000000100000001000110000001000000010001100000010000000100001000000100000001000010000001000000110000100000010000011000001000000011111100000010000000011100000001100000000000000000011000000000000000000100000000000000000001000000000000000000110000000000000000001000000000000000000110000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "('0x3e000e780181c021cc04046040460404204042040c20418203f0201c060000600004000040000c00008000180',\n 6)"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_ = 100\n",
    "tmp = \"\".join(x_test_[id_].astype(\"str\"))[::-1]\n",
    "#print(f\"b{tmp}\")\n",
    "print(f\"b{tmp}\")\n",
    "hex(int(tmp,2)), np.argmax(y_test[id_])#, np.argmax(model.predict(x_train_[id_].reshape(1,-1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    00000000001111111100000000000111\n",
      "1    00011111111000000000111000000110\n",
      "2    00100000000111000000011100000011\n",
      "3    00111000000000110000001100000000\n",
      "4    01000001100001100000000000011000\n",
      "5    01010110000000000001100001000000\n",
      "6    01100000001110000100000000000011\n",
      "7    01111000010000000000001100000100\n",
      "8    10000000000001110000011000000000\n",
      "9    10011111000001100000000011100000\n",
      "10    10100011000000011100000000011001\n",
      "11    10111011110000000001110111111000\n",
      "12    11000000000011111110000000000000\n",
      "13    11010100110000000000000000000000\n",
      "14    11100000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "clipboard = \"\"\n",
    "for i,j in enumerate(range(0,400,28)):\n",
    "    print(f\"{i}    {bin(i)[2:].zfill(4)}{tmp[j:j+28][::-1].zfill(28)[::-1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "CompletedProcess(args='clip', returncode=0)"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "clipboard = \"\"\n",
    "for i,j in enumerate(range(0,400,28)):\n",
    "    clipboard += f\"DATA_IN = 32'b{bin(i)[2:].zfill(4)}{tmp[j:j+28][::-1].zfill(28)[::-1]};\\n\"\n",
    "    clipboard += \"\\t\\t#dly;\"\n",
    "subprocess.run(\"clip\", text=True, input=clipboard)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 28, 28, 1)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000\n",
      "00000000000000000000\n",
      "00000000000000000000\n",
      "00011100000000000000\n",
      "00111111111111111000\n",
      "00000011111111111100\n",
      "00000000000000011000\n",
      "00000000000000011000\n",
      "00000000000000110000\n",
      "00000000000001110000\n",
      "00000000000001100000\n",
      "00000000000011100000\n",
      "00000000000011000000\n",
      "00000000000011000000\n",
      "00000000000110000000\n",
      "00000000001110000000\n",
      "00000000011100000000\n",
      "00000000011000000000\n",
      "00000000110000000000\n",
      "00000001110000000000\n"
     ]
    }
   ],
   "source": [
    "tmp2 = x_test_[id_].reshape(20,20)\n",
    "for i in range(len(tmp2)):\n",
    "    print(\"\".join(np.round(tmp2[i]).astype(\"int\").astype(\"str\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000000000000000000000000\n",
      "0000000111000000000000000000\n",
      "0000001111111111111110000000\n",
      "0000000000111111111111000000\n",
      "0000000000000000000110000000\n",
      "0000000000000000000110000000\n",
      "0000000000000000001100000000\n",
      "0000000000000000011100000000\n",
      "0000000000000000011000000000\n",
      "0000000000000000111000000000\n",
      "0000000000000000110000000000\n",
      "0000000000000000110000000000\n",
      "0000000000000001100000000000\n",
      "0000000000000011100000000000\n",
      "0000000000000111000000000000\n",
      "0000000000000110000000000000\n",
      "0000000000001100000000000000\n",
      "0000000000011100000000000000\n",
      "0000000000011100000000000000\n",
      "0000000000011100000000000000\n",
      "0000000000011000000000000000\n",
      "0000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "tmp2 = x_test[id_].squeeze()\n",
    "for i in range(len(tmp2)):\n",
    "    print(\"\".join(np.round(tmp2[i]).astype(\"int\").astype(\"str\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "((60000, 400), (10000, 400), (60000, 10))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_.shape, x_test_.shape,"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " fc1 (QDense)                (None, 10)                4010      \n",
      "                                                                 \n",
      " relu1 (QActivation)         (None, 10)                0         \n",
      "                                                                 \n",
      " fc2 (QDense)                (None, 10)                110       \n",
      "                                                                 \n",
      " relu2 (QActivation)         (None, 10)                0         \n",
      "                                                                 \n",
      " fc3 (QDense)                (None, 10)                110       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4230 (16.52 KB)\n",
      "Trainable params: 4230 (16.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(keras.Input(shape=input_shape, name=\"input0\"))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    QDense(\n",
    "        10,\n",
    "        name='fc1',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        10,\n",
    "        name='fc2',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
    "model.add(\n",
    "    QDense(\n",
    "        num_classes,\n",
    "        name='fc3',\n",
    "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(activation='softmax', name='softmax'))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.4523 - accuracy: 0.9031 - val_loss: 0.4246 - val_accuracy: 0.9127\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.9046 - val_loss: 0.4204 - val_accuracy: 0.9142\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4470 - accuracy: 0.9055 - val_loss: 0.4192 - val_accuracy: 0.9158\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.9063 - val_loss: 0.4230 - val_accuracy: 0.9122\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.9062 - val_loss: 0.4256 - val_accuracy: 0.9123\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4456 - accuracy: 0.9055 - val_loss: 0.4261 - val_accuracy: 0.9118\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4440 - accuracy: 0.9073 - val_loss: 0.4224 - val_accuracy: 0.9160\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.9091 - val_loss: 0.4242 - val_accuracy: 0.9165\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.9092 - val_loss: 0.4237 - val_accuracy: 0.9117\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4413 - accuracy: 0.9083 - val_loss: 0.4183 - val_accuracy: 0.9148\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4392 - accuracy: 0.9097 - val_loss: 0.4194 - val_accuracy: 0.9175\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.9091 - val_loss: 0.4200 - val_accuracy: 0.9165\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.9099 - val_loss: 0.4183 - val_accuracy: 0.9192\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.9088 - val_loss: 0.4223 - val_accuracy: 0.9130\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.9103 - val_loss: 0.4207 - val_accuracy: 0.9157\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x27f26370df0>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "model.fit(x_train_, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.45980751514434814\n",
      "Test accuracy: 0.9036999940872192\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "save_model(model, \"qModel.keras\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input0, layer type: InputLayer, input shapes: [[None, 400]], output shape: [None, 400]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 400]], output shape: [None, 10]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "-----------------------------------\n",
      "{'Model': {'Precision': 'fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False}, 'LayerName': {'input0': {'Trace': False, 'Precision': {'result': 'ap_uint<1>'}}, 'fc1': {'Trace': False, 'Precision': {'result': 'fixed<16,6>', 'weight': 'fixed<6,1>', 'bias': 'fixed<6,1>'}}, 'fc1_linear': {'Trace': False, 'Precision': {'result': 'fixed<16,6>'}}, 'relu1': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT>'}}, 'fc2': {'Trace': False, 'Precision': {'result': 'fixed<16,6>', 'weight': 'fixed<6,1>', 'bias': 'fixed<6,1>'}}, 'fc2_linear': {'Trace': False, 'Precision': {'result': 'fixed<16,6>'}}, 'relu2': {'Trace': False, 'Precision': {'result': 'ufixed<6,0,RND_CONV,SAT>'}}, 'fc3': {'Trace': False, 'Precision': {'result': 'fixed<16,6>', 'weight': 'fixed<6,1>', 'bias': 'fixed<6,1>'}}, 'fc3_linear': {'Trace': False, 'Precision': {'result': 'fixed<16,6>'}}, 'softmax': {'Trace': False, 'Precision': {'result': 'fixed<16,6>'}, 'exp_table_t': 'ap_fixed<18,8>', 'inv_table_t': 'ap_fixed<18,4>'}}}\n",
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input0, layer type: InputLayer, input shapes: [[None, 400]], output shape: [None, 400]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 400]], output shape: [None, 10]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Layer name: softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find module 'C:\\Users\\Tim\\PycharmProjects\\rs_final\\model_1\\hls4ml_prj\\firmware\\myproject-eC7Bc7Fa.so' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 12\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-----------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m hls_model \u001B[38;5;241m=\u001B[39m hls4ml\u001B[38;5;241m.\u001B[39mconverters\u001B[38;5;241m.\u001B[39mconvert_from_keras_model(\n\u001B[0;32m      9\u001B[0m     model, hls_config\u001B[38;5;241m=\u001B[39mconfig, output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_1/hls4ml_prj\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     10\u001B[0m     part\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxcu250-figd2104-2L-e\u001B[39m\u001B[38;5;124m'\u001B[39m, backend\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVitis\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     11\u001B[0m )\n\u001B[1;32m---> 12\u001B[0m \u001B[43mhls_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ti\\lib\\site-packages\\hls4ml\\model\\graph.py:664\u001B[0m, in \u001B[0;36mModelGraph.compile\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    662\u001B[0m     dlclose_func\u001B[38;5;241m.\u001B[39mrestype \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mc_int\n\u001B[0;32m    663\u001B[0m     dlclose_func(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_top_function_lib\u001B[38;5;241m.\u001B[39m_handle)\n\u001B[1;32m--> 664\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_top_function_lib \u001B[38;5;241m=\u001B[39m \u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcdll\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadLibrary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlib_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ti\\lib\\ctypes\\__init__.py:451\u001B[0m, in \u001B[0;36mLibraryLoader.LoadLibrary\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mLoadLibrary\u001B[39m(\u001B[38;5;28mself\u001B[39m, name):\n\u001B[1;32m--> 451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dlltype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ti\\lib\\ctypes\\__init__.py:373\u001B[0m, in \u001B[0;36mCDLL.__init__\u001B[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001B[0m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_FuncPtr \u001B[38;5;241m=\u001B[39m _FuncPtr\n\u001B[0;32m    372\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 373\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle \u001B[38;5;241m=\u001B[39m \u001B[43m_dlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    375\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle \u001B[38;5;241m=\u001B[39m handle\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Could not find module 'C:\\Users\\Tim\\PycharmProjects\\rs_final\\model_1\\hls4ml_prj\\firmware\\myproject-eC7Bc7Fa.so' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "config['LayerName']['input0']['Precision'] = {'result': 'ap_uint<1>'}  # Set input precision to 1-bit\n",
    "print(\"-----------------------------------\")\n",
    "print(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_1/hls4ml_prj',\n",
    "    part='xcu250-figd2104-2L-e', backend='Vitis'\n",
    ")\n",
    "hls_model.compile()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hls_model.write()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'CSynthesisReport': {'TargetClockPeriod': '5.00',\n  'EstimatedClockPeriod': '4.344',\n  'BestLatency': '10',\n  'WorstLatency': '10',\n  'IntervalMin': '1',\n  'IntervalMax': '1',\n  'BRAM_18K': '12',\n  'DSP': '10',\n  'FF': '2629',\n  'LUT': '61606',\n  'URAM': '0',\n  'AvailableBRAM_18K': '5376',\n  'AvailableDSP': '12288',\n  'AvailableFF': '3456000',\n  'AvailableLUT': '1728000',\n  'AvailableURAM': '1280'}}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PATH'] += os.pathsep + 'D:/ProgramFiles/Xilinx/Vitis_HLS/2023.2/bin'\n",
    "hls_model.build(csim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'hls4ml': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}